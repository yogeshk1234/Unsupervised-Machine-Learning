{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1685455631857}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1><b>Project Type - Un-Supervised Machine learning \n","<h1><b>Contribution - Individual"],"metadata":{"id":"_Gppa-uFw5h4"}},{"cell_type":"markdown","source":["<h1><b>GitHUb Link\n","\n"],"metadata":{"id":"2xeodWPuxPft"}},{"cell_type":"markdown","source":["<h1><b>Project Summary\n","\n","\n","\n","* This project involves analyzing the content available on Netflix, which is a popular streaming service for movies and TV shows. The dataset used in this project includes information about different shows and movies on Netflix as of 2019. The goal of the project is to categorize and group these shows based on their attributes like genre, cast, director, rating, country, and description.\n","\n","* The dataset consisted of 7787 records and 11 attributes. \n","\n","* To start, the project focuses on cleaning the data and performing exploratory data analysis. This helps to ensure the data is organized and any issues are addressed.\n","\n","* Next, the attributes are processed and transformed into a numerical format using a technique called TFIDF vectorization. This allows the project to analyze the textual information effectively. Principal Component Analysis (PCA) is then used to handle the high dimensionality of the data, making it easier to work with.\n","\n","* Two different clustering algorithms, namely K-Means Clustering and Agglomerative Hierarchical Clustering, are employed to group the Netflix content based on similarities between their attributes. The number of clusters is determined using techniques such as the elbow method, silhouette score, and dendrogram.\n","\n","* Lastly, a content-based recommender system is built using the cosine similarity between shows/movies. This system recommends 10 shows/movies to users based on their previous viewing history. By analyzing the attributes of the shows/movies and comparing them to the user's preferences, the recommender system suggests similar content that the user might enjoy.\n","\n","* Overall, this project provides insights into the Netflix dataset and helps users discover shows/movies that align with their preferences, making it easier to find content they will enjoy."],"metadata":{"id":"1JbqmsYHw4ZE"}},{"cell_type":"markdown","source":["<h1><b>Business Context\n","\n","\n","* The business context of this project is to improve the user experience on the Netflix platform by providing personalized content recommendations based on user viewing behavior and preferences. By clustering TV shows and movies into similar groups and integrating external datasets like IMDB ratings and Rotten Tomatoes, the project aims to provide valuable insights into the content's quality and popularity, further enhancing the viewing experience. Ultimately, the project's goal is to drive user engagement and satisfaction on the Netflix platform."],"metadata":{"id":"-tgMtxIIxWoi"}},{"cell_type":"markdown","source":["<h1><b>Problem Statement\n","\n","\n","* The problem statement of this project is to explore and analyze the Netflix dataset to identify patterns and similarities among TV shows and movies. The goal is to cluster the content into groups of similar shows and movies using attributes such as director, cast, country, genre, rating, and description. Additionally, the project aims to integrate external datasets like IMDB ratings and Rotten Tomatoes to provide insights into the content's quality and popularity. The ultimate objective is to build a content-based recommendation system that provides personalized recommendations to users based on their viewing behavior and preferences, ultimately enhancing the user experience on the Netflix platform"],"metadata":{"id":"JmcUhfDqxZ1d"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QDgvAcA1w5G8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import missingno as msno\n","import matplotlib.cm as cm\n","\n","# Word Cloud library\n","from wordcloud import WordCloud, STOPWORDS\n","\n","# library used for textual data prerocessing\n","import string,unicodedata            # string module provides various constants and functions for working with strings\n","string.punctuation                   # punctuation constant is a string containing all the ASCII punctuation characters.\n","import nltk\n","from nltk.corpus import stopwords    # stopwords corpus, which contains a list of commonly used stop words in the English language.\n","nltk.download('stopwords')        # nltk.download('stopwords') is used to download the stopwords corpus from the NLTK library to your local machine. \n","from nltk.stem.snowball import SnowballStemmer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.decomposition import PCA\n","\n","# library used for building recommandation system\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# library used for Clusters impelementation\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.metrics import silhouette_score, silhouette_samples\n","import scipy.cluster.hierarchy as shc\n","\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","df=pd.read_csv(\"/content/drive/MyDrive/capstone project 4/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n","df"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df.head(5)"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","print('Number of rows {} \\n Number of columns {}'.format(df.shape[0],df.shape[1]))"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(len(df[df.duplicated()]))\n","     "],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","df.isna().sum().sort_values(ascending= False).reset_index().rename(columns={'index':'Columns',0:'Null values'})"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(14, 5),dpi=100)\n","msno.bar(df, color = 'blue')\n","plt.title(\"Missing Values in Column\",fontweight=\"bold\",size=16,color='red')\n","plt.show()\n","     \n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":[">The dataset contains information about specific movies.\n","\n",">There are NaN values present in the director, cast, country, date_added, and rating columns.\n","\n",">It is not possible to impute missing values using any method, as the data is specific to each movie.\n","\n",">To avoid losing any data, the decision has been made to impute NaN values with empty space. This approach may not always be the best option, as external sources could potentially provide missing information for some of the columns.Answer Here"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","df.columns\n","     "],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","df.describe(include='all').transpose()\n"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description "],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":[">show_id : Unique ID for every Movie/Show\n","\n",">type : Identifier - Movie/Show\n","\n",">title : Title of the Movie/Show\n","\n",">director : Director of the Movie/Show\n","\n",">cast : Actors involved in the Movie/Show\n","\n",">country : Country where the Movie/Show was produced\n","\n",">date_added : Date it was added on Netflix\n","\n",">release_year : Actual Release year of the Movie/Show\n","\n",">rating : TV Rating of the Movie/Show\n","\n",">duration : Total Duration - in minutes or number of seasons\n","\n",">listed_in : Genre\n","\n",">description: The Summary descriptionAnswer Here"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in df.columns.tolist():\n","  print(\"No. of unique values in \",i,\"is\",df[i].nunique())"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["<h1><B>EDA (Exploratery Data Analysis)\n","\n",">Exploratory Data Analysis (EDA) is a crucial initial step before making any modifications to a dataset or creating a statistical model to address business problems. The EDA process involves summarizing, visualizing, and gaining a deep understanding of the significant characteristics of a dataset. In essence, EDA is aimed at exploring and discovering insights from the data to inform subsequent data processing, modeling, and decision-making activities."],"metadata":{"id":"SstOgfU20dkS"}},{"cell_type":"markdown","source":["<h2>Type Column"],"metadata":{"id":"ruDdwshK06jY"}},{"cell_type":"code","source":["# Number of Movies and TV Shows in the dataset\n","print(df.type.value_counts())\n","print(\" \")"],"metadata":{"id":"YrNFChsA0_Uc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of Movies and TV Shows in the dataset\n","plt.figure(figsize=(10,5),dpi=100)\n","df['type'].value_counts().plot(kind='pie',autopct='%1.2f%%')\n","plt.ylabel('')\n","plt.title('Movies and TV Shows in the dataset',fontsize=16,color='red');"],"metadata":{"id":"abtoODgK2E9E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1>Title Column"],"metadata":{"id":"mNeLCBy52NI9"}},{"cell_type":"code","source":["# Creating and Displaying a Word Cloud Based on Titles in a Pandas Dataframe\n","text = \" \".join(word for word in df['title'])\n","\n","# Create the WordCloud object and generate the word cloud\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(text)\n","\n","# Display the word cloud using matplotlib.pyplot\n","plt.imshow(wordcloud,  interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"ToBmAZbi2JRx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The words like Christmas, Love, World, Man and Story are very comman word which are appear most in movie title column."],"metadata":{"id":"tLf9JrRg2Une"}},{"cell_type":"markdown","source":["<h1><b>Director Column"],"metadata":{"id":"lq5RB08g2b53"}},{"cell_type":"code","source":["# Printing the Number of Directors for Movies and TV Shows Separately\n","\n","print(f\"number of director who  by directed movie : { df[df['type']=='Movie']['director'].value_counts().sum()}\")\n","print(f\"number of director who  by directed TV Show : { df[df['type']=='TV Show']['director'].value_counts().sum()}\")"],"metadata":{"id":"-lGeXKjl2IBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining fig size and axis\n","fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n","\n","# top 10 director who directed TV show\n","show = df[df['type']=='TV Show']['director'].value_counts()[:10].plot(kind='barh', ax=ax[0])\n","show.set_title('top 10 director who directed TV Show', size=16,color='red')\n","\n","# top 10 director who directed movie\n","movie = df[df['type']=='Movie']['director'].value_counts()[:10].plot(kind='barh', ax=ax[1])\n","movie.set_title('top 10 director who directed Movie', size=16,color='red')\n","\n","plt.tight_layout()\n","plt.show()\n","     "],"metadata":{"id":"DiL8G2qw2jxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The director Alastair Fothergill has directed three TV shows, which is the highest number of TV shows directed by any director in the dataset.\n","\n",">Raul Campos and Jan Suter have collaborated directed 18 movies, which is the highest number compared to any other director pair in the dataset. Following them are Marcus Raboy, Jay Karas, and Cathy Garcia-Molina."],"metadata":{"id":"_xUMxteC2sNV"}},{"cell_type":"markdown","source":["<h1><b>Cast Column"],"metadata":{"id":"acCZSimR2yvS"}},{"cell_type":"code","source":["#defing fig size and axis\n","fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n","\n","# top 10 TV shows actor \n","TV_shows = df[df['type']=='TV Show']['cast'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True).value_counts()[:10].plot(kind='barh', ax=ax[0])\n","TV_shows.set_title('Top 10 actors who appeared in Tv shows', size=16,color='red')\n","\n","# top 10 Movie actor \n","movies = df[df['type']=='Movie']['cast'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True).value_counts()[:10].plot(kind='barh', ax=ax[1])\n","movies.set_title('Top 10 actors who appeared in movie', size=16,color='red')\n","\n","plt.tight_layout()\n","plt.show()\n","     "],"metadata":{"id":"l465aQ-l21yi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Takahiro Sakurai, Yuki Kaji and Daisuke Ono played highest role in the TV shows.\n","\n",">Anupam Kher, Shahrukh Khan and Om Puri played highest number of role in the movies."],"metadata":{"id":"91AAMg5S24mt"}},{"cell_type":"code","source":["# Top 10 countries with the highest number movies / TV shows in the dataset\n","plt.figure(figsize=(15,5),dpi=100)\n","df.country.value_counts().nlargest(10).plot(kind='barh')\n","plt.title('Top 10 countries with the highest number of movies / TV shows',fontsize=16,color='red')"],"metadata":{"id":"3pagKbr3293i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The highest number of movies / TV shows were based out of the US, followed by India and UK."],"metadata":{"id":"D8T0-bYq3BQx"}},{"cell_type":"markdown","source":["<h1><b>Released Year Column"],"metadata":{"id":"tzYyhvv33Gdp"}},{"cell_type":"code","source":["fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n","\n","# Univariate analysis\n","hist = sns.histplot(df['release_year'], ax=ax[0])\n","hist.set_title('distribution by released year',fontsize=16,color='red')\n","\n","# Bivariate analysis\n","count = sns.countplot(x=\"release_year\", hue='type', data=df, order=range(2008,2022), ax=ax[1])\n","count.set_title('Number of shows released each year since 2008 that are on Netflix',fontsize=16,color='red')\n","plt.xticks(rotation=90)\n","for p in count.patches:  #adding value count on the top of bar\n","   count.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n","\n","plt.tight_layout()\n","plt.show()\n","     "],"metadata":{"id":"VyNVz4Wo3Kvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Netflix has more new movies and TV shows than old ones.\n","\n",">The company has a consistent focus on adding new shows to its platform.\n","\n",">In 2020, there was a decrease in the number of movies added, but not in the number of TV shows added. This could indicate a shift towards introducing more TV series rather than movies on Netflix."],"metadata":{"id":"EAsYI2ak3Qbr"}},{"cell_type":"markdown","source":["<h1><b>Rating Column"],"metadata":{"id":"jSr5Rjjc3WiE"}},{"cell_type":"code","source":["# Top 10 Rating \n","fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n","plt.suptitle('Top 10 rating given for movie and shows', size=16,color='red', y=1.01)\n","\n","# univariate analysis\n","df['rating'].value_counts()[:10].plot(kind='barh',ax=ax[0])\n","\n","# bivariate analysis\n","graph = sns.countplot(x=\"rating\", data=df, hue='type', order=df['rating'].value_counts().index[0:10], ax=ax[1])\n","plt.xticks(rotation=90)\n","for p in graph.patches:  #adding value count on the top of bar\n","   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"fs2Ry4q53ZhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Most of the movie and tv shows have rating of TV-MA (Mature Audiance) then followed by TV-14 (younger audiance)."],"metadata":{"id":"KaJ9cFch3dZW"}},{"cell_type":"markdown","source":["<h1><b>Duration Column"],"metadata":{"id":"12iJXMbO3hy8"}},{"cell_type":"code","source":["# duration column\n","df['duration']"],"metadata":{"id":"IFBbi5tR3loL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Creating different dataset from duration\n","\n","netflix_shows=df[df['type']=='TV Show']\n","netflix_movies=df[df['type']=='Movie']"],"metadata":{"id":"6OWafhyF3oQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Netflex Movie Duration"],"metadata":{"id":"4-a4wHRb3rEW"}},{"cell_type":"code","source":["# movie duration \n","netflix_movies['duration']=netflix_movies['duration'].str.replace(' min','')\n","netflix_movies['duration']=netflix_movies['duration'].astype(str).astype(int)\n","\n","# Average movie length over the years\n","plt.figure(figsize=(15,5),dpi=100)\n","netflix_movies.groupby('release_year')['duration'].mean().plot(kind='line')\n","plt.title('Average movie length over the years',fontsize=16,color='red')\n","plt.ylabel('Length of movie in minutes')\n","plt.xlabel('Year')\n","     "],"metadata":{"id":"TcSvJ7ZZ3wLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Netflix offers a range of movies on its platform, including those from as far back as 1942.\n","\n",">Movies made in the 1940s had a relatively short duration, according to their plots.\n","\n",">On average, movies made in the 1960s are the longest in length.\n","\n",">The average length of movies has been decreasing steadily since the 2000s."],"metadata":{"id":"FM-FbawU3zNu"}},{"cell_type":"markdown","source":["<h2><B> Netflix TV show Duration"],"metadata":{"id":"ib9t3_-lqNJP"}},{"cell_type":"code","source":["# TV show duration \n","netflix_shows['duration']=netflix_shows['duration'].str.replace(' Season','')\n","netflix_shows['duration']=netflix_shows['duration'].str.replace(' Seasons','')\n","netflix_shows['duration']=netflix_shows['duration'].str.replace('s','')\n","netflix_shows['duration']=netflix_shows['duration'].astype(str).astype(int)\n","\n","# Seasons in each TV show\n","plt.figure(figsize=(15,5),dpi=100)\n","p = sns.countplot(x='duration',data=netflix_shows)\n","plt.title('Number of seasons per TV show distribution',fontsize=16,color='red')\n","\n","for i in p.patches:\n","    p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()),\n","               ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)"],"metadata":{"id":"GCMqeOUgqP-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The TV series in the dataset have a maximum of 16 seasons, but the majority only have one season.\n","\n",">This could suggest that many of the TV shows are relatively new and additional seasons may be in the works.\n","\n",">There are very few TV shows in the dataset with more than 8 seasons."],"metadata":{"id":"jauKN8UOrF_V"}},{"cell_type":"code","source":["# seperating genre from listed_in columns for analysis purpose\n","genres = df['listed_in'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n","\n","# top 10 genre in listed movie/show\n","plt.figure(figsize=(15,5),dpi=100)\n","genres = genres.value_counts()[:10].plot(kind='barh')\n","plt.title('Top 10 genres',fontsize=16,color='red')     "],"metadata":{"id":"-cfnMEWrrJh_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The International movie is the most popular genre followed by dramas and comedies."],"metadata":{"id":"gzV3dB2rrPN9"}},{"cell_type":"markdown","source":["<h1><b>Description"],"metadata":{"id":"XVU-037Y4Rer"}},{"cell_type":"code","source":["# text documents\n","text = \" \".join(word for word in df['description'])\n","\n","# create the word cloud\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(text)\n","\n","# plot the word cloud\n","plt.imshow(wordcloud,  interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"qU-_GRTe4UWx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Most of the comman words present in description column are family, find, life, love, new, world, friend."],"metadata":{"id":"CUcIPtDf4bBv"}},{"cell_type":"markdown","source":["<h1><b>Feature Engenerring & Data Pre-processing"],"metadata":{"id":"dUrGwjJ14fqW"}},{"cell_type":"markdown","source":["<h1>Handling Missing Values"],"metadata":{"id":"Z8SkujrF4rTr"}},{"cell_type":"code","source":["# Missing Data %\n","\n","round(df.isna().mean().sort_values(ascending=False)*100,2)"],"metadata":{"id":"ib0MAG9Z4vSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">For the missing values in the director, cast, and country attributes, the 'empty string' can be used as a replacement.\n","\n",">The percentage of null values in the rating and date_added columns is small, and dropping these values may not significantly impact model building."],"metadata":{"id":"o-ZKRqBc40hX"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","\n","df[['director','cast','country']] = df[['director','cast','country']].fillna(' ')\n","df.dropna(axis=0, inplace=True)"],"metadata":{"id":"_HTpOlyb45kA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking for null values after treating them.\n","\n","df.isna().sum()"],"metadata":{"id":"NMHKWdBY479Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Handling Outlier"],"metadata":{"id":"HU4QQOkb5B5H"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","plt.figure(figsize=(15,5),dpi=100)\n","sns.boxplot(data=df,orient='h');"],"metadata":{"id":"U1j1RuYj5FDf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Outlier handling may not be necessary for textual data as outliers are typically defined in numerical data.\n","\n",">Data cleaning and preprocessing steps are still necessary to ensure the data is ready for model building."],"metadata":{"id":"uRBapnrX5I7u"}},{"cell_type":"markdown","source":["<h1><b>Texual Data Pre-processing"],"metadata":{"id":"BE0tz5yG5SDX"}},{"cell_type":"markdown","source":[">Select the attributes that will be used to cluster the shows.\n","\n",">Perform text preprocessing by removing stopwords and punctuation marks, and converting all textual data to lowercase.\n","\n",">Use stemming to generate a meaningful word out of the corpus of words.\n","\n",">Tokenize the corpus and perform word vectorization.\n","\n",">Apply dimensionality reduction techniques to reduce the dimensionality of the dataset.\n","\n",">Use different algorithms to cluster the movies and determine the optimal number of clusters using various techniques such as the elbow method or silhouette score.\n","\n",">Build the optimal number of clusters and visualize the contents of each cluster using word clouds to gain insights about the characteristics of each cluster."],"metadata":{"id":"fwOqaHjP5XLl"}},{"cell_type":"markdown","source":["<h1><b>Clustering Attributes"],"metadata":{"id":"sxo1ojYP5qaV"}},{"cell_type":"markdown","source":["We will cluster the movie/shows on Netflix based on the following attributes:\n","\n"," >Director\n","\n"," >Cast\n","\n"," >Country\n"," \n"," >Rating\n"," \n"," >Listed in (genres)\n","\n"," >Description"],"metadata":{"id":"2_rszPlV5vMv"}},{"cell_type":"code","source":["# Copying the original dataset for clustering as it does not contain any missing values to handle\n","\n","df1 = df.copy()    "],"metadata":{"id":"-lY5vTPF58Rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating clustering_attributes column using all text column which one is used for model building purpose.\n","\n","df1['clustering_attributes'] = df1['description'] + df1['listed_in'] + df1['rating'] + df1['cast'] + df1['country'] "],"metadata":{"id":"1BFmd3mq59H7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.clustering_attributes[0]"],"metadata":{"id":"HDjZ5jUn5t_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'].head(10)"],"metadata":{"id":"kucY-h-A65nM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Recoving Non ASCII Character"],"metadata":{"id":"ArbZx_xj7DR0"}},{"cell_type":"code","source":["# function to remove non-ascii characters\n","\n","def remove_non_ascii(words):\n","    \"\"\"Function to remove non-ASCII characters\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","        new_words.append(new_word)                    # unicodedata.normalize = convert each string to NFKD form\n","    return new_words                                  # encode = convert string to ASCII format\n","                                                      # decode = convert resulting byte string to regular string format"],"metadata":{"id":"g-PlTmp17JTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove non-ascii characters\n","\n","df1['clustering_attributes'] = remove_non_ascii(df1['clustering_attributes'])"],"metadata":{"id":"bNtGfPGN7P8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'][0]     "],"metadata":{"id":"p5yOWSes7Ucv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'].head(5)"],"metadata":{"id":"T6QoaGVK7XFG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Removing Stopwords And Convert Lower Case"],"metadata":{"id":"RFLpLY1_7sOb"}},{"cell_type":"code","source":["# Download the stop words list if it hasn't been downloaded already\n","nltk.download('stopwords')\n","\n","# Create a set of English stop words\n","stop_words = stopwords.words('english')\n","\n","# Display the stop words\n","print(stop_words)"],"metadata":{"id":"GxmlM9-A7yaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Text Preprocessing: Removing Stopwords and Punctuation Marks, and Stemming.\n","def stopwords(text):\n","    '''a function for removing the stopword and lowercase the each word'''\n","    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n","    # joining the list of words with space separator\n","    return \" \".join(text)"],"metadata":{"id":"Ojfn3V_471KI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing stop words\n","df1['clustering_attributes'] = df1['clustering_attributes'].apply(stopwords)"],"metadata":{"id":"COh8iLOf73nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'][0]"],"metadata":{"id":"QqFhysiE753c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Removing Puntuation"],"metadata":{"id":"3kjzHI4K78w4"}},{"cell_type":"markdown","source":[">Removing punctuation is a common preprocessing step in natural language processing (NLP) tasks. Punctuation marks such as periods, commas, and exclamation points can add noise to the data and can sometimes be treated as separate tokens, which can impact the performance of NLP models."],"metadata":{"id":"DGrmnF8y8Eva"}},{"cell_type":"code","source":["# function to remove punctuations\n","\n","def remove_punctuation(text):\n","    '''a function for removing punctuation'''\n","    # replacing the punctuations with no space, which deletes the punctuation marks.\n","    translator = str.maketrans('', '', string.punctuation)\n","    # return the text stripped of punctuation marks\n","    return text.translate(translator)\n","     "],"metadata":{"id":"gn8yOVA88BHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing punctuation marks\n","df1['clustering_attributes'] = df1['clustering_attributes'].apply(remove_punctuation)"],"metadata":{"id":"DMhKcS_v8LQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'][0]"],"metadata":{"id":"L_C60giZ8No_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Stemming"],"metadata":{"id":"Nkb12YwF8P6R"}},{"cell_type":"markdown","source":[">Stemming operation bundles together words with the same root. For example, the stem operation bundles \"response\" and \"respond\" into the common stem \"respon\".\n","\n",">The SnowballStemmer has been used to generate a meaningful word out of the corpus of words."],"metadata":{"id":"xwLs2Ppb8V0v"}},{"cell_type":"code","source":["# create an object of stemming function\n","stemmer = SnowballStemmer(\"english\")\n","\n","def stemming(text):    \n","    '''a function which stems each word in the given text'''\n","    text = [stemmer.stem(word) for word in text.split()]\n","    return \" \".join(text) "],"metadata":{"id":"ySfueBHX8aKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#performing stemming operation\n","\n","df1['clustering_attributes'] = df1['clustering_attributes'].apply(stemming)"],"metadata":{"id":"7v5bUSHm8cr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['clustering_attributes'][0]  "],"metadata":{"id":"yubbJiH38fAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Text Vectorization"],"metadata":{"id":"WlcLe3sS8h68"}},{"cell_type":"code","source":["# extract the tfid representation matrix of the text data\n","tfid_vectorizer= TfidfVectorizer(stop_words='english', lowercase=False, max_features = 10000)  # max features = 10000 to prevent system from crashing\n","tfid_matrix = tfid_vectorizer.fit_transform(df1['clustering_attributes'])        \n","\n","# collect the tfid matrix in numpy array\n","array = tfid_matrix.toarray()  "],"metadata":{"id":"x_rmHP8c8lFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Print Shape and Data Type of a NumPy Array\n","\n","print(array)\n","print(f'shape of the vector : {array.shape}')\n","print(f'datatype : {type(array)}')"],"metadata":{"id":"8Xp4yLH98sXm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>Dimensionility Reduction"],"metadata":{"id":"NF10kX-d8uzu"}},{"cell_type":"markdown","source":[">Dimensionality reduction is the process of reducing the number of features or dimensions in a dataset while preserving as much information as possible.\n","\n",">PCA (Principal Component Analysis) can be used to reduce the dimensionality of the data."],"metadata":{"id":"EXA2iQoK803E"}},{"cell_type":"code","source":["# using PCA to reduce dimensionality\n","\n","pca = PCA(random_state=0)\n","pca.fit(array)  "],"metadata":{"id":"4EPAtuT_8z-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explained variance for different number of components\n","\n","plt.figure(figsize=(10,5))\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.title('PCA - Cumulative explained variance vs number of components',fontsize=16,color='red')\n","plt.xlabel('number of components')\n","plt.ylabel('cumulative explained variance');"],"metadata":{"id":"PlOWdF-A8SaQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">After performing PCA, it was found that ~7600 components can explain 100% of the variance in the data.\n","\n",">More than 80% of the variance can be explained by just 4000 components.\n","\n",">Selecting the top 4000 components can help simplify the model and reduce dimensionality while still capturing more than 80% of the variance."],"metadata":{"id":"u_MhJ7pc9Cu0"}},{"cell_type":"code","source":["# reducing the dimensions to 4000 using pca\n","pca = PCA(n_components=4000,random_state=0)\n","pca.fit(array)"],"metadata":{"id":"d_lidDSv9KCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transformed features\n","X = pca.transform(array)\n","\n","# shape of transformed vectors\n","X.shape"],"metadata":{"id":"JoTgFSeD9M2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The dimensionality of the data has been successfully reduced using PCA."],"metadata":{"id":"DirDSpxd9Pp9"}},{"cell_type":"markdown","source":["<h1><B>Ml Model Implimentation"],"metadata":{"id":"gl7f9ks49Tis"}},{"cell_type":"markdown","source":["#####<b><h2>1. K-Means Clustering"],"metadata":{"id":"mhQI5ScX9ZST"}},{"cell_type":"markdown","source":[">K-means clustering is a popular unsupervised machine learning algorithm that divides a dataset into a predefined number of clusters. Since it is an unsupervised algorithm, it does not rely on labeled examples to learn about the data. To determine the optimal number of clusters for the K-means algorithm, we can use the elbow curve and Silhouette score visualization techniques."],"metadata":{"id":"7LgbPPEw9oax"}},{"cell_type":"markdown","source":["<b>Elbow method to find best value of k\n","\n"],"metadata":{"id":"WaZhbsso9t5f"}},{"cell_type":"markdown","source":[">The elbow curve is a plot of the sum of squared distances between each point and the centroid in a cluster against the number of clusters. As the number of clusters increases, the sum of squared distances generally decreases. The \"elbow\" point on the curve represents the optimal number of clusters, beyond which the decrease in sum of squared distances is not significant."],"metadata":{"id":"FIZTxKY4-Dhd"}},{"cell_type":"code","source":["# The Elbow Method for Determining Optimal Number of Clusters\n","\n","sum_of_sq_dist =[]\n","for i in range(1,20):\n","  # Initialize the k-means model with the current value of i\n","  kmeans = KMeans(n_clusters=i,init='k-means++',random_state=0)\n","  # Fit the model to the data\n","  kmeans.fit(X)\n","  # Compute the sum of squared errors for the model\n","  sum_of_sq_dist.append(kmeans.inertia_)\n","\n","# Plot the value of SSE\n","number_clusters = range(1,20)\n","plt.figure(figsize=(15,5),dpi=100)\n","plt.plot(number_clusters,sum_of_sq_dist)\n","plt.title('The Elbow Method for optimal K',fontsize=16,color='red')\n","plt.xlabel('Number of clusters (K)')\n","plt.ylabel('Sum of squared distances')\n","plt.show()"],"metadata":{"id":"Fl1_AVMI-KIL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Select the number of clusters as 10, as no drastic difference is visible after that."],"metadata":{"id":"cQekuu2v-ZSg"}},{"cell_type":"code","source":["# KMeans Clustering Visualization of Data Points\n","plt.figure(figsize=(10,6), dpi=150)\n","\n","kmeans= KMeans(n_clusters=10, init= 'k-means++', random_state=0)\n","kmeans.fit(X)\n","\n","#predict the labels of clusters.\n","label = kmeans.fit_predict(X)\n","#Getting unique labels\n","unique_labels = np.unique(label)\n"," \n","#plotting the results:\n","for i in unique_labels:\n","    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\n","plt.legend()\n","plt.title('KMeans Clustering Visualization',fontsize=16,color='red')\n","plt.show()"],"metadata":{"id":"_vS_anhQ-juu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><b>Silhouette score method to find the optimal value of k"],"metadata":{"id":"y383mJ8R-p8L"}},{"cell_type":"code","source":["# Initialize a list to store the silhouette score for each value of k\n","silhouette_scr = []\n","\n","for k in range(2, 15):\n","  # Initialize the k-means model with the current value of k\n","  kmeans = KMeans(n_clusters=k, init='k-means++', random_state=0)\n","  # Fit the model to the data\n","  kmeans.fit(X)\n","  # Predict the cluster labels for each point in the data\n","  labels = kmeans.labels_\n","  # silhouette score for the model\n","  score = silhouette_score(X, labels)\n","  silhouette_scr.append(score)\n","  \n","# Plot the Silhouette analysis\n","plt.figure(figsize=(15,5),dpi=100)\n","plt.plot(range(2,15), silhouette_scr)\n","plt.xlabel('Number of clusters (K)') \n","plt.ylabel('Silhouette score')\n","plt.title('Silhouette analysis For Optimal k',fontsize=16,color='red')\n","plt.show()"],"metadata":{"id":"rTN4hhW6-tEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The highest Silhouette score is obtained for 6 clusters."],"metadata":{"id":"n5rEBjDC-x79"}},{"cell_type":"markdown","source":["<h2><b>Building clusters using the k-means algorithm:"],"metadata":{"id":"baStZaWvj8b9"}},{"cell_type":"code","source":["# Clustering the data into 6 clusters\n","\n","kmeans = KMeans(n_clusters=6, init='k-means++', random_state=0)\n","kmeans.fit(X)"],"metadata":{"id":"TyehhkiB-0GO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation metrics - distortion, Silhouette score\n","\n","kmeans_distortion = kmeans.inertia_\n","kmeans_silhouette_score = silhouette_score(X, kmeans.labels_)\n","\n","print((kmeans_distortion, kmeans_silhouette_score))\n","     "],"metadata":{"id":"0ivIfbGLkIPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding a kmeans cluster number attribute\n","df1['kmeans_cluster'] = kmeans.labels_"],"metadata":{"id":"5gqiTrm9kLKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of movies and tv shows in each cluster\n","\n","plt.figure(figsize=(15,5),dpi=100)\n","graph = sns.countplot(x='kmeans_cluster',data=df1, hue='type')\n","plt.title('Number of movies and TV shows in each cluster',fontsize=16,color='red')\n","\n","# adding value count on the top of bar\n","for p in graph.patches:\n","   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n","     "],"metadata":{"id":"KaqnzmlWkOFX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Successfully built 6 clusters using the k-means clustering algorithm."],"metadata":{"id":"AhdyfEr6kSnr"}},{"cell_type":"markdown","source":["<h2><b>Building wordclouds for different clusters built:"],"metadata":{"id":"WBnlPNb-kUJe"}},{"cell_type":"code","source":["# Building a wordcloud for the movie descriptions\n","\n","def kmeans_worldcloud(cluster_num):\n","  comment_words = ''\n","  stopwords = set(STOPWORDS)\n","\n","  # iterate through the csv file\n","  for val in df1[df1['kmeans_cluster']==cluster_num].description.values:\n","      \n","      # typecaste each val to string\n","      val = str(val)\n","\n","      # split the value\n","      tokens = val.split()\n","      \n","      # Converts each token into lowercase\n","      for i in range(len(tokens)):\n","          tokens[i] = tokens[i].lower()\n","      \n","      comment_words += \" \".join(tokens)+\" \"\n","\n","  wordcloud = WordCloud(width = 700, height = 700,\n","                  background_color ='white',\n","                  stopwords = stopwords,\n","                  min_font_size = 10).generate(comment_words)\n","\n","\n","  # plot the WordCloud image                      \n","  plt.figure(figsize = (10,5), facecolor = None,dpi=100)\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout(pad = 0)"],"metadata":{"id":"40uRSkOdkZYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wordcloud for cluster 0\n","\n","kmeans_worldcloud(0)"],"metadata":{"id":"gA6b9hGtkfqI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Keywords observed in cluster 0: comdedian, stage, first, special, life, deliver, funny, humor, share"],"metadata":{"id":"akcuKuUdk41c"}},{"cell_type":"code","source":["# Wordcloud for cluster 1\n","kmeans_worldcloud(1)"],"metadata":{"id":"GaM6c4CPk78L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 1: life, love, family, word, find, new, young, three, team"],"metadata":{"id":"nuZTmhTDk8vn"}},{"cell_type":"code","source":["# Wordcloud for cluster 2\n","\n","kmeans_worldcloud(2)    "],"metadata":{"id":"88B4Mk6MlBXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 2:   life, new, find, word, family, become, girl, learn, school"],"metadata":{"id":"hrYF7Nu0lCyA"}},{"cell_type":"code","source":["# Wordcloud for cluster 3\n","\n","kmeans_worldcloud(3)"],"metadata":{"id":"ZXIqBuFblHca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 3: muscial,friend, love, band, music, documentary, one, young, life"],"metadata":{"id":"QB75n_LOlKo9"}},{"cell_type":"code","source":["# Wordcloud for cluster 4\n","\n","kmeans_worldcloud(4)"],"metadata":{"id":"80gLYgTmlMy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 4: documentary, series, family, live, animal, explore, filmmaker, world, live"],"metadata":{"id":"KwLGZfAJlPrs"}},{"cell_type":"code","source":["# Wordcloud for cluster 5\n","\n","kmeans_worldcloud(5)"],"metadata":{"id":"dANqbmI6lVuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 5: find, life, family, man, woman,friend, brother, young, three"],"metadata":{"id":"qbVUQY3alZCr"}},{"cell_type":"markdown","source":["<h1><b>Hierarcial Clustering"],"metadata":{"id":"WQ7cdF5ulfic"}},{"cell_type":"markdown","source":[">The agglomerative (hierarchical) clustering algorithm is employed to construct clusters. This approach involves merging clusters that are similar, starting with each sample as a single-sample cluster, and building a hierarchy of clusters from the bottom up. To determine the optimal number of clusters, a dendrogram can be visualized when using the agglomerative (hierarchical) clustering algorithm."],"metadata":{"id":"PE4yNPL5lrD-"}},{"cell_type":"code","source":["# Building a dendogram to decide on the number of clusters\n","\n","plt.figure(figsize=(15,5),dpi=100)  \n","dend = shc.dendrogram(shc.linkage(X, method='ward'))\n","plt.title('Dendrogram',fontsize=16,color='red')\n","plt.xlabel('Netflix Shows')\n","plt.ylabel('Distance')\n","plt.axhline(y= 4.1, color='r', linestyle='--')"],"metadata":{"id":"L9mg6UYtltxg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Using the agglomerative clustering algorithm, it is possible to construct 7 clusters at a distance of 4.1 units."],"metadata":{"id":"WWlhfMdDlyxW"}},{"cell_type":"code","source":["# Fitting hierarchical clustering model\n","\n","hierarchical = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')  \n","hierarchical.fit_predict(X)   "],"metadata":{"id":"pcJcMOlHl1nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of movies and tv shows in each cluster\n","plt.figure(figsize=(15,5),dpi=100)\n","graph = sns.countplot(x='hierarchical_cluster',data=df1, hue='type')\n","plt.title('Number of movies and tv shows in each cluster - Hierarchical Clustering',fontsize=16,color='red')\n","\n","# adding value count on the top of bar\n","for p in graph.patches:\n","   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))"],"metadata":{"id":"dQhOC7Orl4df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The Agglomerative (hierarchical) clustering algorithm was utilized to construct 7 clusters successfully."],"metadata":{"id":"8qvxICvUl7Ec"}},{"cell_type":"markdown","source":["<H2><B>Building wordclouds for different clusters built:"],"metadata":{"id":"8Yws0wP0l9-M"}},{"cell_type":"code","source":["# Building a wordcloud for the movie descriptions\n","def hierarchical_worldcloud(cluster_num):\n","  comment_words = ''\n","  stopwords = set(STOPWORDS)\n","\n","  # iterate through the csv file\n","  for val in df1[df1['hierarchical_cluster']==cluster_num].description.values:\n","      \n","      # typecaste each val to string\n","      val = str(val)\n","\n","      # split the value\n","      tokens = val.split()\n","      \n","      # Converts each token into lowercase\n","      for i in range(len(tokens)):\n","          tokens[i] = tokens[i].lower()\n","      \n","      comment_words += \" \".join(tokens)+\" \"\n","\n","  wordcloud = WordCloud(width = 700, height = 700,\n","                  background_color ='white',\n","                  stopwords = stopwords,\n","                  min_font_size = 10).generate(comment_words)\n","\n","\n","  # plot the WordCloud image                      \n","  plt.figure(figsize = (15,5), facecolor = None,dpi=100)\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout(pad = 0)"],"metadata":{"id":"cxRBC_p2mAFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wordcloud for cluster 0\n","hierarchical_worldcloud(0)"],"metadata":{"id":"eXaluQ7NmGCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 0: find, life, family, new, take, friend, become, love, live"],"metadata":{"id":"SSD2WCEGmIl8"}},{"cell_type":"code","source":["# Wordcloud for cluster 1\n","hierarchical_worldcloud(1) "],"metadata":{"id":"B3gUnrUEmLM-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 1: life, love, family, world, find, friend, young, must, crime"],"metadata":{"id":"f5E_nBSAmL7I"}},{"cell_type":"code","source":["# Wordcloud for cluster 2\n","hierarchical_worldcloud(2)"],"metadata":{"id":"3ydCnqI0mOnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 2: life, man, women, young, group, family, young, find, polics"],"metadata":{"id":"crBFFAGdmQ28"}},{"cell_type":"code","source":["# Wordcloud for cluster 3\n","hierarchical_worldcloud(3)"],"metadata":{"id":"1vKJ5aetmUgv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 3: father, love, family, man, find, friend, indian, women, india"],"metadata":{"id":"jYJBSkTVmXEI"}},{"cell_type":"code","source":["# Wordcloud for cluster 4\n","hierarchical_worldcloud(4)     "],"metadata":{"id":"gdNTHNK_mZsf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 4: natural, creature, examine, planet, series, world, earth, explore, planet"],"metadata":{"id":"08E5kq5QmcB8"}},{"cell_type":"code","source":["# Wordcloud for cluster 5\n","hierarchical_worldcloud(5)"],"metadata":{"id":"jWdr4SpamgXV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 5: new, love, life, korean, women, two, group, help, world"],"metadata":{"id":"OdT0HWrAmjMm"}},{"cell_type":"code","source":["# Wordcloud for cluster 6\n","hierarchical_worldcloud(6)"],"metadata":{"id":"N71cvFp7mnma"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Keywords observed in cluster 6: young, years, world, must, life, new, demon, group, battle"],"metadata":{"id":"MMdWl5kGmqLU"}},{"cell_type":"markdown","source":["<H1><B>Content based recommender system:"],"metadata":{"id":"CB2fud0zmqz2"}},{"cell_type":"markdown","source":["<B>Content-based recommendation systems make recommendations to users by utilizing the similarities between items. These recommendation systems suggest products or items to users based on their descriptions or features, and they determine the degree of similarity between the products by analyzing their descriptions."],"metadata":{"id":"D8gmQpsfm7_C"}},{"cell_type":"code","source":["# veryfying index\n","df1[['show_id', 'title', 'clustering_attributes']]     "],"metadata":{"id":"ZRGAVYahmvx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As shown in the above dataframe, the total number of rows present in our dataframe is 7770. However, the last index appears as 7786 due to the dropping of some rows while handling null values."],"metadata":{"id":"yNDQ_MAFnH4s"}},{"cell_type":"code","source":["# defining a new df for building a recommender system\n","recommender_df = df1.copy()     "],"metadata":{"id":"MzrPJ64pnAVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reseting index\n","recommender_df.reset_index(inplace=True)\n","\n","# checking reset index \n","recommender_df[['show_id', 'title', 'clustering_attributes']]     "],"metadata":{"id":"Ncu7MZrinMtD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The index has been successfully reset, and the dataset is now ready to be used for building a content-based recommendation system."],"metadata":{"id":"gx65gbAgnPum"}},{"cell_type":"code","source":["# dropping show-id and index column\n","recommender_df.drop(columns=['index', 'show_id'], inplace=True)  "],"metadata":{"id":"03xzShywnSS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calling out transformed array after performing PCA for dimenssionality reduction.\n","X   "],"metadata":{"id":"ZzUG4svXnUyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity\n","similarity = cosine_similarity(X)\n","similarity     "],"metadata":{"id":"ipYnbXuhnXFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function for list down top 10 recommended movie on the basis of cosine similarity score.\n","def recommend(movie):\n","  try:\n","    '''\n","    This function list down top ten movies on the basis of similarity score for that perticular movie.\n","    '''\n","    # Empty list\n","    recommend_content = []   \n","    # find out index position\n","    index = recommender_df[recommender_df['title'] == movie].index[0]\n","    # sorting on the basis of simliarity score, In order to find out distaces from recommended one\n","    distances = sorted(list(enumerate(similarity[index])), reverse=True, key=lambda x:x[1])\n","    # printing Statement\n","    print(f\"If you liked '{movie}', you may also enjoy: \\n\")\n","    # listing top ten recommenaded movie\n","    for i in distances[1:11]:\n","      recommend_content.append(df1.iloc[i[0]].title)\n","    return recommend_content\n","  except:\n","     return 'Invalid Entry'"],"metadata":{"id":"XBrowYngnld8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend('Naruto')"],"metadata":{"id":"k1glY759nn5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend('A Man Called God')"],"metadata":{"id":"fXO2pcomnq-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend('Avenger')"],"metadata":{"id":"FooE2FBRntzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend('Phir Hera Pheri')"],"metadata":{"id":"h-QCl3tpnwK1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<H1><B>Conclusion : "],"metadata":{"id":"LA0jeo6SnzvN"}},{"cell_type":"markdown","source":["* In this project, I worked on a text clustering problem in which I clustered Netflix shows into groups with similar attributes. Here's a summary of my findings and actions taken:\n","\n","* The dataset had 7787 records and 11 attributes. I performed exploratory data analysis and handled missing values.\n","\n","* I discovered that Netflix has more movies than TV shows, and the number of shows on the platform is growing rapidly. Most shows are produced in the United States.\n","\n","* I chose to cluster the data based on attributes such as director, cast, country, genre, rating, and description. I tokenized, preprocessed, and vectorized these attributes using TFIDF Vectorizer, resulting in 10,000 attributes.\n","\n","* To reduce dimensionality, I utilized Principal Component Analysis (PCA), with 4,000 components capturing over 80% of variance.\n","\n","* I used K-Means Clustering to build the initial clusters, with the optimal number of clusters being 6 as determined by the elbow method and Silhouette score analysis.\n","\n","* Agglomerative clustering was used to create clusters, with 7 being the optimal number determined by visualizing the dendrogram.\n","\n","* Finally, I developed a content-based recommender system using cosine similarity on the similarity matrix. The recommender system recommends the top 10 shows based on the type of show the user has watched."],"metadata":{"id":"Ln2U-gkmn6i6"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}